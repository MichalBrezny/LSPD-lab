{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Lab 3: River Profiles and Drainage Metrics\n",
    "\n",
    "**Landscape and Surface Processes Dynamics**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "- Learn how to extract and analyze drainage networks from Digital Elevation Models (DEMs)\n",
    "- Practice slope–area analysis for understanding landscape evolution\n",
    "- Extract and analyze longitudinal river profiles\n",
    "- Identify knickpoints and understand their tectonic and climatic significance\n",
    "- Compare concavity indices between tectonically active and stable basins\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "This notebook uses Python libraries for geospatial analysis. We'll work with:\n",
    "- **richdem**: DEM analysis and hydrological processing\n",
    "- **numpy/scipy**: Numerical computations\n",
    "- **matplotlib**: Visualization\n",
    "- **rasterio**: Raster data handling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install richdem rasterio matplotlib numpy scipy elevation\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import richdem as rd\n",
    "from scipy import ndimage\n",
    "import rasterio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_acquisition"
   },
   "source": [
    "## Data Acquisition\n",
    "\n",
    "For this lab, you can either:\n",
    "1. Upload your own DEM file\n",
    "2. Use sample DEM data\n",
    "3. Download DEM from online sources (e.g., SRTM, ASTER)\n",
    "\n",
    "We'll demonstrate with a sample dataset approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Option 1: Upload your DEM file\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# dem_file = list(uploaded.keys())[0]\n",
    "\n",
    "# Option 2: Create synthetic DEM for demonstration\n",
    "# We'll create a simple synthetic landscape for demonstration\n",
    "def create_synthetic_dem(size=500, seed=42):\n",
    "    \"\"\"Create a synthetic DEM with realistic features\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create base topography with gradient\n",
    "    x = np.linspace(0, 1, size)\n",
    "    y = np.linspace(0, 1, size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Base elevation: higher in upper right, lower in lower left\n",
    "    base = 1000 * (0.7 * X + 0.3 * Y) + 500\n",
    "    \n",
    "    # Add some random noise for realism\n",
    "    noise = np.random.randn(size, size) * 10\n",
    "    \n",
    "    # Add some valley features\n",
    "    for i in range(3):\n",
    "        center_x = np.random.uniform(0.2, 0.8)\n",
    "        center_y = np.random.uniform(0.2, 0.8)\n",
    "        valley = -100 * np.exp(-((X - center_x)**2 + (Y - center_y)**2) / 0.05)\n",
    "        base += valley\n",
    "    \n",
    "    dem = base + noise\n",
    "    return dem\n",
    "\n",
    "# Create synthetic DEM\n",
    "dem_array = create_synthetic_dem()\n",
    "\n",
    "# Convert to richdem format\n",
    "dem = rd.rdarray(dem_array, no_data=-9999)\n",
    "\n",
    "print(f\"DEM loaded successfully!\")\n",
    "print(f\"DEM shape: {dem.shape}\")\n",
    "print(f\"Elevation range: {np.min(dem):.2f} to {np.max(dem):.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_dem"
   },
   "outputs": [],
   "source": [
    "# Visualize the DEM\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(dem, cmap='terrain', aspect='auto')\n",
    "plt.colorbar(label='Elevation (m)', shrink=0.8)\n",
    "plt.title('Digital Elevation Model (DEM)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X (pixels)')\n",
    "plt.ylabel('Y (pixels)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_header"
   },
   "source": [
    "## Step 1: DEM Preprocessing - Pit Filling\n",
    "\n",
    "**Theory:**\n",
    "DEMs often contain \"pits\" or \"sinks\" - local depressions that prevent water from flowing continuously downslope. These can be:\n",
    "- Artifacts from data acquisition/processing\n",
    "- Real features (lakes, closed depressions)\n",
    "\n",
    "**Pit filling** removes these depressions to ensure continuous flow routing, which is essential for drainage network extraction.\n",
    "\n",
    "**Algorithm:** We'll use the priority-flood algorithm (Barnes et al., 2014), which efficiently fills depressions while preserving topographic details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pit_filling"
   },
   "outputs": [],
   "source": [
    "# Create a copy for comparison\n",
    "dem_original = dem.copy()\n",
    "\n",
    "# Fill depressions using priority-flood algorithm\n",
    "print(\"Filling depressions...\")\n",
    "dem_filled = rd.FillDepressions(dem, epsilon=True, in_place=False)\n",
    "print(\"Depression filling complete!\")\n",
    "\n",
    "# Calculate the difference to see what was filled\n",
    "fill_depth = dem_filled - dem_original\n",
    "\n",
    "print(f\"Maximum fill depth: {np.max(fill_depth):.2f} m\")\n",
    "print(f\"Pixels modified: {np.sum(fill_depth > 0)} ({100*np.sum(fill_depth > 0)/dem.size:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_filling"
   },
   "outputs": [],
   "source": [
    "# Visualize pit filling results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original DEM\n",
    "im1 = axes[0].imshow(dem_original, cmap='terrain', aspect='auto')\n",
    "axes[0].set_title('Original DEM', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('X (pixels)')\n",
    "axes[0].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im1, ax=axes[0], label='Elevation (m)', shrink=0.8)\n",
    "\n",
    "# Filled DEM\n",
    "im2 = axes[1].imshow(dem_filled, cmap='terrain', aspect='auto')\n",
    "axes[1].set_title('Filled DEM', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('X (pixels)')\n",
    "axes[1].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im2, ax=axes[1], label='Elevation (m)', shrink=0.8)\n",
    "\n",
    "# Fill depth\n",
    "im3 = axes[2].imshow(fill_depth, cmap='YlOrRd', aspect='auto')\n",
    "axes[2].set_title('Fill Depth', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('X (pixels)')\n",
    "axes[2].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im3, ax=axes[2], label='Depth (m)', shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_header"
   },
   "source": [
    "## Step 2: Extract Drainage Network with Threshold Contributing Area\n",
    "\n",
    "**Theory:**\n",
    "Drainage networks form where sufficient water accumulates to initiate channels. We extract the network by:\n",
    "1. Computing **flow direction** (D8 or D-infinity algorithm)\n",
    "2. Calculating **flow accumulation** (upstream contributing area)\n",
    "3. Applying a **threshold** to define channel initiation\n",
    "\n",
    "**Threshold selection:**\n",
    "- Too low: Dense network with many spurious channels\n",
    "- Too high: Misses smaller tributaries\n",
    "- Typical: 0.1-1 km² depending on climate and lithology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flow_accumulation"
   },
   "outputs": [],
   "source": [
    "# Calculate flow accumulation\n",
    "print(\"Computing flow accumulation...\")\n",
    "accum = rd.FlowAccumulation(dem_filled, method='D8')\n",
    "print(\"Flow accumulation complete!\")\n",
    "\n",
    "# Assuming 30m pixel resolution (typical for SRTM)\n",
    "pixel_area_km2 = (30 * 30) / 1e6  # Convert m² to km²\n",
    "accum_area = accum * pixel_area_km2\n",
    "\n",
    "print(f\"Maximum contributing area: {np.max(accum_area):.2f} km²\")\n",
    "print(f\"Flow accumulation computed for {dem.size} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_network"
   },
   "outputs": [],
   "source": [
    "# Define threshold for channel initiation (in km²)\n",
    "threshold_area_km2 = 0.5  # Adjust based on your study area\n",
    "\n",
    "# Extract drainage network\n",
    "channel_network = accum_area > threshold_area_km2\n",
    "\n",
    "print(f\"Channel network extracted with threshold: {threshold_area_km2} km²\")\n",
    "print(f\"Number of channel pixels: {np.sum(channel_network)}\")\n",
    "print(f\"Drainage density: {np.sum(channel_network) * 30 / (dem.size * 900):.3f} km/km²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_network"
   },
   "outputs": [],
   "source": [
    "# Visualize drainage network\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Flow accumulation (log scale for better visualization)\n",
    "im1 = axes[0].imshow(np.log10(accum + 1), cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Flow Accumulation (log₁₀)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('X (pixels)')\n",
    "axes[0].set_ylabel('Y (pixels)')\n",
    "plt.colorbar(im1, ax=axes[0], label='log₁₀(cells)', shrink=0.8)\n",
    "\n",
    "# Drainage network overlay on DEM\n",
    "axes[1].imshow(dem_filled, cmap='terrain', alpha=0.6, aspect='auto')\n",
    "axes[1].imshow(np.ma.masked_where(~channel_network, channel_network), \n",
    "               cmap='Blues', alpha=0.8, aspect='auto')\n",
    "axes[1].set_title(f'Drainage Network (threshold = {threshold_area_km2} km²)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('X (pixels)')\n",
    "axes[1].set_ylabel('Y (pixels)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_header"
   },
   "source": [
    "## Step 3: Calculate Stream Orders (Strahler)\n",
    "\n",
    "**Theory:**\n",
    "Stream ordering classifies river segments hierarchically:\n",
    "\n",
    "**Strahler Method:**\n",
    "- Headwater streams = order 1\n",
    "- When two streams of order n meet → order n+1\n",
    "- When streams of different orders meet → higher order continues\n",
    "\n",
    "**Applications:**\n",
    "- Understanding drainage basin organization\n",
    "- Predicting discharge and sediment yield\n",
    "- Geomorphic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stream_order"
   },
   "outputs": [],
   "source": [
    "def calculate_strahler_order(dem_filled, accum, threshold):\n",
    "    \"\"\"\n",
    "    Calculate Strahler stream order.\n",
    "    Simplified implementation for demonstration.\n",
    "    \"\"\"\n",
    "    # Create channel network\n",
    "    channels = accum > threshold\n",
    "    \n",
    "    # Initialize stream order array\n",
    "    stream_order = np.zeros_like(dem_filled, dtype=int)\n",
    "    \n",
    "    # Simple approximation: use flow accumulation as proxy\n",
    "    # Higher accumulation = higher order\n",
    "    accum_channels = accum.copy()\n",
    "    accum_channels[~channels] = 0\n",
    "    \n",
    "    # Define order thresholds based on accumulation percentiles\n",
    "    valid_accum = accum_channels[channels]\n",
    "    if len(valid_accum) > 0:\n",
    "        p33 = np.percentile(valid_accum, 33)\n",
    "        p66 = np.percentile(valid_accum, 66)\n",
    "        \n",
    "        stream_order[channels & (accum_channels <= p33)] = 1\n",
    "        stream_order[channels & (accum_channels > p33) & (accum_channels <= p66)] = 2\n",
    "        stream_order[channels & (accum_channels > p66)] = 3\n",
    "    \n",
    "    return stream_order\n",
    "\n",
    "# Calculate stream orders\n",
    "print(\"Calculating Strahler stream orders...\")\n",
    "stream_orders = calculate_strahler_order(dem_filled, accum, threshold_area_km2 / pixel_area_km2)\n",
    "print(\"Stream order calculation complete!\")\n",
    "\n",
    "# Statistics\n",
    "for order in range(1, 4):\n",
    "    count = np.sum(stream_orders == order)\n",
    "    if count > 0:\n",
    "        print(f\"Order {order}: {count} pixels ({count * 30 / 1000:.2f} km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_orders"
   },
   "outputs": [],
   "source": [
    "# Visualize stream orders\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(dem_filled, cmap='terrain', alpha=0.5, aspect='auto')\n",
    "\n",
    "# Create custom colormap for stream orders\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['none', '#3498db', '#2ecc71', '#e74c3c']  # transparent, blue, green, red\n",
    "cmap_orders = ListedColormap(colors)\n",
    "\n",
    "im = plt.imshow(np.ma.masked_where(stream_orders == 0, stream_orders), \n",
    "                cmap=cmap_orders, vmin=0, vmax=3, alpha=0.9, aspect='auto')\n",
    "cbar = plt.colorbar(im, ticks=[0, 1, 2, 3], label='Stream Order', shrink=0.8)\n",
    "cbar.set_ticklabels(['None', '1st', '2nd', '3rd'])\n",
    "\n",
    "plt.title('Strahler Stream Orders', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X (pixels)')\n",
    "plt.ylabel('Y (pixels)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_header"
   },
   "source": [
    "## Step 4: Generate Slope-Area Plots\n",
    "\n",
    "**Theory:**\n",
    "The slope-area relationship is fundamental to understanding landscape evolution:\n",
    "\n",
    "$S = k_s A^{-\\theta}$\n",
    "\n",
    "Where:\n",
    "- $S$ = local channel slope\n",
    "- $A$ = drainage area\n",
    "- $k_s$ = steepness index (reflects rock uplift/erosion rate)\n",
    "- $\\theta$ = concavity index (typically 0.3-0.6)\n",
    "\n",
    "**Log-log transformation:**\n",
    "$\\log(S) = \\log(k_s) - \\theta \\log(A)$\n",
    "\n",
    "**Interpretation:**\n",
    "- Slope decreases with increasing drainage area\n",
    "- Deviations indicate tectonic activity, lithologic boundaries, or base-level changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "calculate_slope"
   },
   "outputs": [],
   "source": [
    "# Calculate slope\n",
    "print(\"Calculating slope...\")\n",
    "slope = rd.TerrainAttribute(dem_filled, attrib='slope_riserun')\n",
    "print(\"Slope calculation complete!\")\n",
    "\n",
    "print(f\"Slope range: {np.min(slope):.4f} to {np.max(slope):.4f} m/m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slope_area_analysis"
   },
   "outputs": [],
   "source": [
    "# Extract slope and area for channel pixels\n",
    "channel_mask = channel_network & (slope > 0) & (accum_area > 0)\n",
    "slope_channels = slope[channel_mask]\n",
    "area_channels = accum_area[channel_mask]\n",
    "\n",
    "# Remove outliers for better visualization\n",
    "valid = (slope_channels > np.percentile(slope_channels, 1)) & \\\n",
    "        (slope_channels < np.percentile(slope_channels, 99))\n",
    "slope_clean = slope_channels[valid]\n",
    "area_clean = area_channels[valid]\n",
    "\n",
    "# Perform log-log regression\n",
    "log_area = np.log10(area_clean)\n",
    "log_slope = np.log10(slope_clean)\n",
    "\n",
    "# Linear fit: log(S) = log(ks) - theta * log(A)\n",
    "coeffs = np.polyfit(log_area, log_slope, 1)\n",
    "theta = -coeffs[0]  # Concavity index\n",
    "log_ks = coeffs[1]  # Log of steepness index\n",
    "ks = 10**log_ks\n",
    "\n",
    "print(f\"Slope-Area Analysis Results:\")\n",
    "print(f\"Concavity index (θ): {theta:.3f}\")\n",
    "print(f\"Steepness index (ks): {ks:.6f}\")\n",
    "print(f\"Number of channel points analyzed: {len(area_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_slope_area"
   },
   "outputs": [],
   "source": [
    "# Create slope-area plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Regular scale with density\n",
    "axes[0].hexbin(area_clean, slope_clean, gridsize=50, cmap='viridis', \n",
    "               mincnt=1, alpha=0.8, edgecolors='none')\n",
    "axes[0].set_xlabel('Drainage Area (km²)', fontsize=11)\n",
    "axes[0].set_ylabel('Channel Slope (m/m)', fontsize=11)\n",
    "axes[0].set_title('Slope-Area Relationship', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-log scale with regression line\n",
    "axes[1].hexbin(area_clean, slope_clean, gridsize=50, cmap='viridis',\n",
    "               xscale='log', yscale='log', mincnt=1, alpha=0.8, edgecolors='none')\n",
    "\n",
    "# Add regression line\n",
    "area_fit = np.logspace(np.log10(area_clean.min()), np.log10(area_clean.max()), 100)\n",
    "slope_fit = ks * area_fit**(-theta)\n",
    "axes[1].plot(area_fit, slope_fit, 'r-', linewidth=2, \n",
    "             label=f'S = {ks:.2e} × A$^{{-{theta:.2f}}}$')\n",
    "\n",
    "axes[1].set_xlabel('Drainage Area (km²)', fontsize=11)\n",
    "axes[1].set_ylabel('Channel Slope (m/m)', fontsize=11)\n",
    "axes[1].set_title('Log-Log Slope-Area Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10, loc='best')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_header"
   },
   "source": [
    "## Step 5: Extract Longitudinal Profiles for 2-3 Rivers\n",
    "\n",
    "**Theory:**\n",
    "Longitudinal river profiles show elevation versus distance from source:\n",
    "- **Equilibrium profile:** Smooth, concave-up curve\n",
    "- **Knickpoints:** Sharp slope breaks indicating disequilibrium\n",
    "- **Profile evolution:** Reflects balance between uplift and erosion\n",
    "\n",
    "**Profile equation:**\n",
    "$z(x) = z_b + k_s \\cdot (\\frac{A_0}{A(x)})^{\\theta}$\n",
    "\n",
    "Where $z$ = elevation, $x$ = distance, $A$ = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_profiles"
   },
   "outputs": [],
   "source": [
    "def extract_river_profile(dem, accum, start_point, min_accum=100):\n",
    "    \"\"\"\n",
    "    Extract longitudinal profile by following flow from a starting point.\n",
    "    \n",
    "    Parameters:\n",
    "    - dem: filled DEM\n",
    "    - accum: flow accumulation\n",
    "    - start_point: (row, col) starting location\n",
    "    - min_accum: minimum accumulation to continue\n",
    "    \"\"\"\n",
    "    profile_points = []\n",
    "    current = start_point\n",
    "    visited = set()\n",
    "    \n",
    "    # D8 flow direction: 8 neighboring cells (NW, N, NE, W, E, SW, S, SE)\n",
    "    # Offsets in (row, col) format for the 8 cardinal and diagonal directions\n",
    "    neighbors = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
    "    \n",
    "    while True:\n",
    "        r, c = current\n",
    "        \n",
    "        # Check bounds and if visited\n",
    "        if (r < 0 or r >= dem.shape[0] or c < 0 or c >= dem.shape[1] or \n",
    "            current in visited):\n",
    "            break\n",
    "        \n",
    "        visited.add(current)\n",
    "        \n",
    "        # Record point\n",
    "        elevation = dem[r, c]\n",
    "        accumulation = accum[r, c]\n",
    "        profile_points.append((r, c, elevation, accumulation))\n",
    "        \n",
    "        # Stop if accumulation too low\n",
    "        if accumulation < min_accum:\n",
    "            break\n",
    "        \n",
    "        # Find downstream neighbor (highest accumulation)\n",
    "        max_accum = -1\n",
    "        next_point = None\n",
    "        \n",
    "        for dr, dc in neighbors:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if (0 <= nr < dem.shape[0] and 0 <= nc < dem.shape[1] and \n",
    "                (nr, nc) not in visited):\n",
    "                if accum[nr, nc] > max_accum and accum[nr, nc] > accumulation:\n",
    "                    max_accum = accum[nr, nc]\n",
    "                    next_point = (nr, nc)\n",
    "        \n",
    "        if next_point is None:\n",
    "            break\n",
    "        \n",
    "        current = next_point\n",
    "        \n",
    "        # Limit profile length\n",
    "        if len(profile_points) > 1000:\n",
    "            break\n",
    "    \n",
    "    if len(profile_points) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Convert to arrays\n",
    "    profile_array = np.array(profile_points)\n",
    "    rows = profile_array[:, 0]\n",
    "    cols = profile_array[:, 1]\n",
    "    elevations = profile_array[:, 2]\n",
    "    accumulations = profile_array[:, 3]\n",
    "    \n",
    "    # Calculate distances (assuming 30m pixels)\n",
    "    distances = np.zeros(len(rows))\n",
    "    for i in range(1, len(rows)):\n",
    "        dr = rows[i] - rows[i-1]\n",
    "        dc = cols[i] - cols[i-1]\n",
    "        distances[i] = distances[i-1] + np.sqrt(dr**2 + dc**2) * 30  # meters\n",
    "    \n",
    "    distances_km = distances / 1000  # Convert to km\n",
    "    \n",
    "    return {\n",
    "        'distance': distances_km,\n",
    "        'elevation': elevations,\n",
    "        'accumulation': accumulations,\n",
    "        'rows': rows,\n",
    "        'cols': cols\n",
    "    }\n",
    "\n",
    "# Find starting points (high accumulation points)\n",
    "# Select 3 points with high accumulation but spatially separated\n",
    "print(\"Finding river starting points...\")\n",
    "high_accum_mask = accum > np.percentile(accum[channel_network], 90)\n",
    "high_accum_points = np.argwhere(high_accum_mask)\n",
    "\n",
    "# Select 3 diverse starting points\n",
    "if len(high_accum_points) >= 3:\n",
    "    indices = np.linspace(0, len(high_accum_points)-1, 3, dtype=int)\n",
    "    start_points = [tuple(high_accum_points[i]) for i in indices]\n",
    "else:\n",
    "    start_points = [tuple(p) for p in high_accum_points[:3]]\n",
    "\n",
    "# Extract profiles\n",
    "profiles = []\n",
    "for i, start in enumerate(start_points):\n",
    "    print(f\"Extracting profile {i+1} from point {start}...\")\n",
    "    profile = extract_river_profile(dem_filled, accum, start, min_accum=100)\n",
    "    if profile is not None:\n",
    "        profiles.append(profile)\n",
    "        print(f\"  Profile length: {profile['distance'][-1]:.2f} km\")\n",
    "        print(f\"  Elevation change: {profile['elevation'][0] - profile['elevation'][-1]:.2f} m\")\n",
    "\n",
    "print(f\"\\nExtracted {len(profiles)} river profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_profiles"
   },
   "outputs": [],
   "source": [
    "# Plot longitudinal profiles\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Profile locations on map\n",
    "axes[0].imshow(dem_filled, cmap='terrain', alpha=0.7, aspect='auto')\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, profile in enumerate(profiles):\n",
    "    axes[0].plot(profile['cols'], profile['rows'], \n",
    "                color=colors[i % 3], linewidth=2, \n",
    "                label=f'River {i+1}', alpha=0.8)\n",
    "axes[0].set_title('River Profile Locations', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('X (pixels)')\n",
    "axes[0].set_ylabel('Y (pixels)')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Longitudinal profiles\n",
    "for i, profile in enumerate(profiles):\n",
    "    axes[1].plot(profile['distance'], profile['elevation'], \n",
    "                color=colors[i % 3], linewidth=2, marker='o', \n",
    "                markersize=3, label=f'River {i+1}', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Distance from Source (km)', fontsize=11)\n",
    "axes[1].set_ylabel('Elevation (m)', fontsize=11)\n",
    "axes[1].set_title('Longitudinal River Profiles', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6_header"
   },
   "source": [
    "## Step 6: Identify Knickpoints and Classify Causes\n",
    "\n",
    "**Theory:**\n",
    "Knickpoints are sharp breaks in river profile slope that indicate:\n",
    "\n",
    "**1. Tectonic knickpoints:**\n",
    "- Fault scarps\n",
    "- Differential uplift\n",
    "- Stationary, steep\n",
    "\n",
    "**2. Lithologic knickpoints:**\n",
    "- Resistant rock layers\n",
    "- Stationary\n",
    "- Related to geology\n",
    "\n",
    "**3. Base-level knickpoints:**\n",
    "- Sea-level changes\n",
    "- Lake formation/drainage\n",
    "- Migrate upstream\n",
    "\n",
    "**Detection methods:**\n",
    "- Slope breaks\n",
    "- Curvature analysis\n",
    "- Chi-plot deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detect_knickpoints"
   },
   "outputs": [],
   "source": [
    "def detect_knickpoints(profile, threshold_factor=1.5):\n",
    "    \"\"\"\n",
    "    Detect knickpoints using slope change analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - profile: dictionary with distance and elevation\n",
    "    - threshold_factor: sensitivity for knickpoint detection\n",
    "    \n",
    "    Returns:\n",
    "    - indices of knickpoint locations\n",
    "    \"\"\"\n",
    "    distance = profile['distance']\n",
    "    elevation = profile['elevation']\n",
    "    \n",
    "    if len(distance) < 5:\n",
    "        return []\n",
    "    \n",
    "    # Calculate local slope\n",
    "    local_slope = np.abs(np.gradient(elevation, distance))\n",
    "    \n",
    "    # Calculate second derivative (curvature)\n",
    "    curvature = np.abs(np.gradient(local_slope, distance))\n",
    "    \n",
    "    # Smooth to reduce noise\n",
    "    window = min(5, len(curvature) // 10)\n",
    "    if window > 1:\n",
    "        curvature_smooth = ndimage.uniform_filter1d(curvature, window)\n",
    "    else:\n",
    "        curvature_smooth = curvature\n",
    "    \n",
    "    # Find peaks in curvature\n",
    "    threshold = np.mean(curvature_smooth) + threshold_factor * np.std(curvature_smooth)\n",
    "    knickpoints = []\n",
    "    \n",
    "    for i in range(1, len(curvature_smooth) - 1):\n",
    "        if (curvature_smooth[i] > threshold and \n",
    "            curvature_smooth[i] > curvature_smooth[i-1] and \n",
    "            curvature_smooth[i] > curvature_smooth[i+1]):\n",
    "            knickpoints.append(i)\n",
    "    \n",
    "    return knickpoints\n",
    "\n",
    "# Detect knickpoints for each profile\n",
    "all_knickpoints = []\n",
    "for i, profile in enumerate(profiles):\n",
    "    knickpoints = detect_knickpoints(profile, threshold_factor=1.5)\n",
    "    all_knickpoints.append(knickpoints)\n",
    "    print(f\"River {i+1}: {len(knickpoints)} knickpoints detected\")\n",
    "    for kp_idx in knickpoints:\n",
    "        dist = profile['distance'][kp_idx]\n",
    "        elev = profile['elevation'][kp_idx]\n",
    "        print(f\"  Knickpoint at {dist:.2f} km, elevation {elev:.1f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_knickpoints"
   },
   "outputs": [],
   "source": [
    "# Visualize profiles with knickpoints\n",
    "fig, axes = plt.subplots(len(profiles), 1, figsize=(14, 5*len(profiles)))\n",
    "\n",
    "if len(profiles) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "colors_rivers = ['red', 'blue', 'green']\n",
    "\n",
    "for i, (profile, knickpoints) in enumerate(zip(profiles, all_knickpoints)):\n",
    "    # Plot profile\n",
    "    axes[i].plot(profile['distance'], profile['elevation'], \n",
    "                color=colors_rivers[i % 3], linewidth=2, \n",
    "                marker='o', markersize=3, label='River profile', alpha=0.7)\n",
    "    \n",
    "    # Mark knickpoints\n",
    "    if len(knickpoints) > 0:\n",
    "        kp_dist = profile['distance'][knickpoints]\n",
    "        kp_elev = profile['elevation'][knickpoints]\n",
    "        axes[i].scatter(kp_dist, kp_elev, color='red', s=200, \n",
    "                       marker='*', edgecolors='black', linewidths=1.5,\n",
    "                       label='Knickpoints', zorder=5)\n",
    "    \n",
    "    axes[i].set_xlabel('Distance from Source (km)', fontsize=11)\n",
    "    axes[i].set_ylabel('Elevation (m)', fontsize=11)\n",
    "    axes[i].set_title(f'River {i+1} - Longitudinal Profile with Knickpoints', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knickpoint_classification"
   },
   "source": [
    "### Knickpoint Classification Guidelines\n",
    "\n",
    "To classify the causes of detected knickpoints, consider:\n",
    "\n",
    "**1. Morphological characteristics:**\n",
    "- **Slope**: Very steep = likely tectonic/lithologic; Moderate = possible base-level\n",
    "- **Shape**: Angular = tectonic; Rounded = erosional or base-level\n",
    "- **Height**: Large vertical drop = major control (fault, resistant lithology)\n",
    "\n",
    "**2. Spatial patterns:**\n",
    "- **Alignment**: Multiple rivers at same elevation = base-level change\n",
    "- **Stationary**: Same location across tributaries = lithologic or tectonic\n",
    "- **Progressive**: Upstream migration = base-level wave\n",
    "\n",
    "**3. External data (if available):**\n",
    "- Geologic maps (lithology)\n",
    "- Fault maps (tectonics)\n",
    "- Sea-level/lake history (base-level)\n",
    "\n",
    "**Exercise:** For each detected knickpoint, examine its characteristics and propose a likely cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7_header"
   },
   "source": [
    "## Step 7: Exercise - Compare Concavity Index (θ) for Tectonically Active vs. Stable Basins\n",
    "\n",
    "**Background:**\n",
    "The concavity index (θ) reflects the balance between erosion processes:\n",
    "\n",
    "**Typical values:**\n",
    "- **Tectonically active basins**: θ ≈ 0.4-0.6 (steeper, more convex profiles)\n",
    "- **Stable basins**: θ ≈ 0.3-0.5 (gentler, more concave profiles)\n",
    "- **Detachment-limited**: θ ≈ 0.5\n",
    "- **Transport-limited**: θ ≈ 1.0\n",
    "\n",
    "**Factors affecting θ:**\n",
    "- Uplift rate\n",
    "- Rock erodibility\n",
    "- Climate (precipitation)\n",
    "- Sediment supply\n",
    "\n",
    "**Task:**\n",
    "Compare θ values between different rivers or regions to infer tectonic activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise_concavity"
   },
   "outputs": [],
   "source": [
    "def calculate_profile_concavity(profile):\n",
    "    \"\"\"\n",
    "    Calculate concavity index for a single river profile.\n",
    "    Uses slope-area relationship: S = ks * A^(-theta)\n",
    "    \"\"\"\n",
    "    distance = profile['distance']\n",
    "    elevation = profile['elevation']\n",
    "    area = profile['accumulation'] * (30 * 30 / 1e6)  # Convert to km²\n",
    "    \n",
    "    # Calculate local slope\n",
    "    slope = np.abs(np.gradient(elevation, distance * 1000))  # m/m\n",
    "    \n",
    "    # Remove invalid values\n",
    "    valid = (slope > 0) & (area > 0) & np.isfinite(slope) & np.isfinite(area)\n",
    "    slope_valid = slope[valid]\n",
    "    area_valid = area[valid]\n",
    "    \n",
    "    if len(slope_valid) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    # Log-log regression\n",
    "    log_area = np.log10(area_valid)\n",
    "    log_slope = np.log10(slope_valid)\n",
    "    \n",
    "    # Remove outliers\n",
    "    q1_s, q99_s = np.percentile(log_slope, [1, 99])\n",
    "    q1_a, q99_a = np.percentile(log_area, [1, 99])\n",
    "    outlier_mask = ((log_slope > q1_s) & (log_slope < q99_s) & \n",
    "                   (log_area > q1_a) & (log_area < q99_a))\n",
    "    \n",
    "    if np.sum(outlier_mask) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    log_area_clean = log_area[outlier_mask]\n",
    "    log_slope_clean = log_slope[outlier_mask]\n",
    "    \n",
    "    # Fit: log(S) = log(ks) - theta * log(A)\n",
    "    coeffs = np.polyfit(log_area_clean, log_slope_clean, 1)\n",
    "    theta = -coeffs[0]\n",
    "    ks = 10**coeffs[1]\n",
    "    \n",
    "    return theta, ks\n",
    "\n",
    "# Calculate concavity for each river\n",
    "print(\"\\n=== Concavity Index Analysis ===\")\n",
    "print(\"\\nRiver-specific concavity indices:\")\n",
    "concavities = []\n",
    "\n",
    "for i, profile in enumerate(profiles):\n",
    "    theta, ks = calculate_profile_concavity(profile)\n",
    "    if theta is not None:\n",
    "        concavities.append(theta)\n",
    "        print(f\"River {i+1}:\")\n",
    "        print(f\"  Concavity index (θ) = {theta:.3f}\")\n",
    "        print(f\"  Steepness index (ks) = {ks:.6f}\")\n",
    "        \n",
    "        # Interpret\n",
    "        if theta > 0.5:\n",
    "            regime = \"transport-limited / tectonically stable\"\n",
    "        elif theta > 0.35:\n",
    "            regime = \"detachment-limited / moderately active\"\n",
    "        else:\n",
    "            regime = \"highly active tectonic setting\"\n",
    "        print(f\"  Interpretation: {regime}\")\n",
    "\n",
    "if len(concavities) > 0:\n",
    "    print(f\"\\nMean concavity index: {np.mean(concavities):.3f} ± {np.std(concavities):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise_comparison"
   },
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Concavity comparison\n",
    "if len(concavities) > 0:\n",
    "    river_names = [f'River {i+1}' for i in range(len(concavities))]\n",
    "    colors_bar = ['red', 'blue', 'green'][:len(concavities)]\n",
    "    \n",
    "    bars = axes[0].bar(river_names, concavities, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "    axes[0].axhline(y=0.5, color='orange', linestyle='--', linewidth=2, \n",
    "                    label='Typical detachment-limited (θ=0.5)')\n",
    "    axes[0].axhline(y=0.35, color='red', linestyle='--', linewidth=2, \n",
    "                    label='Active tectonic threshold (θ=0.35)')\n",
    "    axes[0].set_ylabel('Concavity Index (θ)', fontsize=11)\n",
    "    axes[0].set_title('Concavity Index Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim(0, max(concavities) * 1.2 if max(concavities) > 0 else 1)\n",
    "\n",
    "# Plot 2: Reference comparison with literature\n",
    "reference_data = {\n",
    "    'Himalaya\\n(active)': 0.42,\n",
    "    'Appalachians\\n(stable)': 0.55,\n",
    "    'Taiwan\\n(very active)': 0.38,\n",
    "    'Amazon\\n(cratonic)': 0.65,\n",
    "    'This Study\\n(mean)': np.mean(concavities) if len(concavities) > 0 else 0\n",
    "}\n",
    "\n",
    "regions = list(reference_data.keys())\n",
    "theta_values = list(reference_data.values())\n",
    "colors_ref = ['tomato', 'lightblue', 'orangered', 'lightgreen', 'gold']\n",
    "\n",
    "bars2 = axes[1].bar(regions, theta_values, color=colors_ref, alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1].set_ylabel('Concavity Index (θ)', fontsize=11)\n",
    "axes[1].set_title('Comparison with Global Reference Basins', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise_discussion"
   },
   "source": [
    "### Discussion Questions\n",
    "\n",
    "Based on your analysis, consider:\n",
    "\n",
    "1. **What do the concavity values suggest about tectonic activity in your study area?**\n",
    "   - θ < 0.4: Potentially active tectonics, rapid uplift\n",
    "   - θ = 0.4-0.6: Moderate activity, balanced conditions\n",
    "   - θ > 0.6: Stable, cratonic conditions\n",
    "\n",
    "2. **How consistent are θ values between different rivers?**\n",
    "   - High consistency: Regional control (climate, lithology, tectonics)\n",
    "   - High variability: Local controls, different erosional regimes\n",
    "\n",
    "3. **How do your values compare with reference basins?**\n",
    "   - Similar to active orogens (Himalaya, Taiwan): Suggest active tectonics\n",
    "   - Similar to stable regions (Appalachians, Amazon): Suggest stability\n",
    "\n",
    "4. **What other factors could affect concavity?**\n",
    "   - Lithologic variations\n",
    "   - Climate gradients\n",
    "   - Glacial history\n",
    "   - Base-level changes\n",
    "\n",
    "5. **How do knickpoints relate to concavity patterns?**\n",
    "   - Knickpoints may represent transient adjustment\n",
    "   - Profile segments between knickpoints may have different θ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Conclusion and Summary\n",
    "\n",
    "In this lab, you have:\n",
    "\n",
    "1. ✓ **Preprocessed DEM data** using pit-filling algorithms\n",
    "2. ✓ **Extracted drainage networks** using flow accumulation and threshold area\n",
    "3. ✓ **Calculated stream orders** using the Strahler method\n",
    "4. ✓ **Generated slope-area plots** and derived concavity/steepness indices\n",
    "5. ✓ **Extracted longitudinal river profiles** for multiple rivers\n",
    "6. ✓ **Identified knickpoints** and discussed their possible causes\n",
    "7. ✓ **Compared concavity indices** to interpret tectonic activity\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **River profiles** record landscape evolution and tectonic history\n",
    "- **Slope-area relationships** reveal erosion processes and uplift rates\n",
    "- **Concavity index (θ)** is a diagnostic tool for tectonic activity\n",
    "- **Knickpoints** mark disequilibrium and landscape transience\n",
    "- **Drainage metrics** provide quantitative constraints on geomorphic processes\n",
    "\n",
    "---\n",
    "\n",
    "### Further Exploration\n",
    "\n",
    "To extend this analysis:\n",
    "\n",
    "1. **Use real DEM data** from your study area (SRTM, ASTER, LiDAR)\n",
    "2. **Perform chi (χ) analysis** for more robust profile comparison\n",
    "3. **Integrate with geologic/tectonic maps** for knickpoint classification\n",
    "4. **Compare multiple basins** across tectonic boundaries\n",
    "5. **Model landscape evolution** using numerical models (e.g., Landlab, FastScape)\n",
    "6. **Analyze temporal changes** using multi-temporal DEMs\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "**Key Papers:**\n",
    "- Barnes, R., et al. (2014). \"Priority-flood: An optimal depression-filling algorithm.\" *Computers & Geosciences*.\n",
    "- Perron, J.T., & Royden, L. (2013). \"An integral approach to bedrock river profile analysis.\" *Earth Surface Processes and Landforms*.\n",
    "- Wobus, C., et al. (2006). \"Tectonics from topography: Procedures, promise, and pitfalls.\" *GSA Special Papers*.\n",
    "- Whipple, K.X., & Tucker, G.E. (1999). \"Dynamics of the stream-power river incision model.\" *JGR: Solid Earth*.\n",
    "\n",
    "**Software:**\n",
    "- RichDEM: https://github.com/r-barnes/richdem\n",
    "- TopoToolbox: https://topotoolbox.wordpress.com/\n",
    "- LSDTopoTools: https://lsdtopotools.github.io/\n",
    "\n",
    "---\n",
    "\n",
    "**End of Lab 3**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab3_River_Profiles_and_Drainage_Metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
